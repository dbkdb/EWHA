{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA10F_AE10E.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\dabin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\dabin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (1.14.1)\n",
      "Downloading lightgbm-4.5.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 11.2 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# GridSearchCV 실행\u001b[39;00m\n\u001b[0;32m     45\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     46\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     47\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madj R2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m )\n\u001b[1;32m---> 55\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 최적의 하이퍼파라미터 출력\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:1189\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1174\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    296\u001b[0m     cb(\n\u001b[0;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    305\u001b[0m     )\n\u001b[1;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4135\u001b[0m _safe_call(\n\u001b[1;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4140\u001b[0m )\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA10F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 700, 'num_leaves': 200}\n",
      "Best Score: -0.009765659486947192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA10F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DaBin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 700, 'num_leaves': 200}\n",
      "Best Score: -0.009829931822618269\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA30F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 700, 'num_leaves': 200}\n",
      "Best Score: -0.009940968228103773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA30F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 200}\n",
      "Best Score: -0.009569943752073184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA30F_PCA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 200}\n",
      "Best Score: -0.009489553390311252\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/PCA30F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100, 'num_leaves': 200}\n",
      "Best Score: -0.009891828536059877\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/AE10F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,500],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: -0.0099659057776966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/AE10F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 100, 'num_leaves': 200}\n",
      "Best Score: -0.009628797346458284\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/AE10F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010032413467004678\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/AE30F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 200}\n",
      "Best Score: -0.010032960087548642\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/AE30F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: -0.010174256346564284\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/UA10F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: -0.010206234588495866\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/UA10F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010030261209206482\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/UA10F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010030232337190989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/UA30F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010267114560850738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/UA30F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010267114560850738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('/content/drive/MyDrive/계특 프로젝트/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('/content/drive/MyDrive/계특 프로젝트/병합 데이터/Firm_58/58F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500,700],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010267114560850738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('/content/drive/MyDrive/계특 프로젝트/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('/content/drive/MyDrive/계특 프로젝트/병합 데이터/Firm_58/58F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010267114560850738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('/content/drive/MyDrive/계특 프로젝트/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('/content/drive/MyDrive/계특 프로젝트/병합 데이터/Firm_58/58F_PCA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [2,4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1, 0.2],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'num_leaves': 50}\n",
      "Best Score: -0.010267114560850738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('/content/drive/MyDrive/계특 프로젝트/firm_500.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('/content/drive/MyDrive/계특 프로젝트/병합 데이터/Firm_58/58F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_depth': [4, 8,10],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'num_leaves': [50,100,200],       #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0.00985159,\n",
    "0.00976565,\n",
    "0.00982993,\n",
    "0.00994096,\n",
    "0.00956994,\n",
    "0.00948955,\n",
    "0.00989182,\n",
    "0.00996590,\n",
    "0.00962879,\n",
    "0.0100324,\n",
    "0.0100329,\n",
    "0.0101742,\n",
    "0.0102062,\n",
    "0.0100302,\n",
    "0.0102671,\n",
    "0.01006672,\n",
    "0.01003023,\n",
    "0.00985321,\n",
    "0.00989323,\n",
    "0.00935612,\n",
    "0.01006672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00935612,\n",
       " 0.00948955,\n",
       " 0.00956994,\n",
       " 0.00962879,\n",
       " 0.00976565,\n",
       " 0.00982993,\n",
       " 0.00985159,\n",
       " 0.00985321,\n",
       " 0.00989182,\n",
       " 0.00989323,\n",
       " 0.00994096,\n",
       " 0.0099659,\n",
       " 0.0100302,\n",
       " 0.01003023,\n",
       " 0.0100324,\n",
       " 0.0100329,\n",
       " 0.01006672,\n",
       " 0.01006672,\n",
       " 0.0101742,\n",
       " 0.0102062,\n",
       " 0.0102671]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00935612,\n",
       " 0.00948955,\n",
       " 0.00956994,\n",
       " 0.00962879,\n",
       " 0.00976565,\n",
       " 0.00982993,\n",
       " 0.00985159,\n",
       " 0.00985321,\n",
       " 0.00989182,\n",
       " 0.00989323,\n",
       " 0.00994096,\n",
       " 0.0099659,\n",
       " 0.0100302,\n",
       " 0.01003023,\n",
       " 0.0100324,\n",
       " 0.0100329,\n",
       " 0.01006672,\n",
       " 0.01006672,\n",
       " 0.0101742,\n",
       " 0.0102062,\n",
       " 0.0102671]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
