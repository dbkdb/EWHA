{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "firm = pd.read_csv(\"C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_500_new.csv\")\n",
    "macro = pd.read_csv(\"C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/macroeconomic_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_df = firm.drop(['Founded','date','gvkey'],axis = 1)\n",
    "firm_df.head()\n",
    "\n",
    "\n",
    "firm_new = firm_df.drop([ 1244,  1245,  1580,  1835,  1836,  1837,  1838,  1839,  1840,\n",
    "         1841,  6859,  6860, 10795, 10796, 10797, 10798, 10799, 10800,\n",
    "        11707, 11708, 11709, 11710, 11711, 11712, 11713, 14308, 14309,\n",
    "        14310, 14613, 14614, 17652, 17653, 20027, 20295, 20598, 21168,\n",
    "        22034, 22035, 22036, 22339, 22340, 22341, 22342, 22343, 26400,\n",
    "        26401, 26402, 27570, 27687, 27688, 29160, 29161, 33512, 33513,\n",
    "        33514, 35810, 35811, 35812, 36737, 36738, 36739, 36740, 36741,\n",
    "        37938, 37939, 37940, 37941, 37942, 37943, 37944, 42602, 44677,\n",
    "        44678, 45357, 45358, 46853, 46854, 46855, 46856, 46857, 47071,\n",
    "        47072, 50080, 50081, 50082, 50083, 50084, 50085, 50086, 50389,\n",
    "        50390, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 52055,\n",
    "        52056, 52057, 52058, 52059, 52362, 52363, 52364, 52365, 52366,\n",
    "        52669, 52670, 52671, 52672, 55392, 55393, 57459, 57460, 57461,\n",
    "        58600, 58601, 58602, 58603, 60977, 60978, 60979, 61915, 61916,\n",
    "        61917, 63715, 63716, 64318, 64319, 64921, 64922, 64923, 64924,\n",
    "        64925, 69966, 69967, 69968, 69969, 69970, 69971, 69972, 72537,\n",
    "        72538, 72824, 74594, 74595, 75191, 75192, 76113, 76114, 76115,\n",
    "        76116, 76117, 76118, 76119, 78474, 78475, 78476, 78477, 78478,\n",
    "        82573, 82574, 82575, 82576, 83246, 83247, 84601, 84602, 84603,\n",
    "        84604, 84605, 84606, 84607],axis = 0)\n",
    "\n",
    "firm_new.reset_index()\n",
    "firm_new.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_new.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_firm_new = firm_new.drop(['ret','month','year','ticker'],axis = 1)\n",
    "\n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_firm_new = scaler.fit_transform(scaled_firm_new)\n",
    "scaled_firm_new = pd.DataFrame(scaled_firm_new)\n",
    "\n",
    "scaled_firm_new.columns = firm_new.drop(['ret','month','year','ticker'],axis = 1).columns\n",
    "\n",
    "scaled_firm_new['ret'] = firm_new['ret']\n",
    "scaled_firm_new['month'] = firm_new['month']\n",
    "scaled_firm_new['year'] = firm_new['year']\n",
    "scaled_firm_new['ticker'] = firm_new['ticker']\n",
    "\n",
    "\n",
    "scaled_firm_new.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/scaled_firm_new.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_new.csv')\n",
    "macro = pd.read_csv(\"C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/macroeconomic_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105492, 60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 125)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_2 = firm.drop(['ret','month','year','ticker'],axis=1)\n",
    "macro_2 = macro.drop(['month','year','sasdate'],axis=1)\n",
    "firm_58 = firm.drop(['ret','ticker'],axis = 1)\n",
    "macro_all = macro.drop(['sasdate'],axis = 1)\n",
    "\n",
    "\n",
    "#PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_com = 10\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(firm_2)\n",
    "PCA_firm_10 = pd.DataFrame(pca_array, index = firm_2.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 30\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(firm_2)\n",
    "PCA_firm_30 = pd.DataFrame(pca_array, index = firm_2.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 10\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(macro_2)\n",
    "PCA_macro_10 = pd.DataFrame(pca_array, index = macro_2.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 50\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(macro_2)\n",
    "PCA_macro_50 = pd.DataFrame(pca_array, index = macro_2.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 80\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(macro_2)\n",
    "PCA_macro_80 = pd.DataFrame(pca_array, index = macro_2.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_firm_10['month'] = firm_58['month']\n",
    "PCA_firm_10['year'] = firm_58['year']\n",
    "PCA_firm_30['month'] = firm_58['month']\n",
    "PCA_firm_30['year'] = firm_58['year']\n",
    "\n",
    "PCA_macro_10['month'] = macro_all['month']\n",
    "PCA_macro_50['month'] = macro_all['month']\n",
    "PCA_macro_80['month'] = macro_all['month']\n",
    "\n",
    "PCA_macro_10['year'] = macro_all['year']\n",
    "PCA_macro_50['year'] = macro_all['year']\n",
    "PCA_macro_80['year'] = macro_all['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA10F_PCA10E =  pd.merge(PCA_firm_10,PCA_macro_10,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_PCA50E =  pd.merge(PCA_firm_10,PCA_macro_50,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_PCA80E =  pd.merge(PCA_firm_10,PCA_macro_80,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_PCA124E =  pd.merge(PCA_firm_10,macro_all,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "PCA30F_PCA10E =  pd.merge(PCA_firm_30,PCA_macro_10,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_PCA50E =  pd.merge(PCA_firm_30,PCA_macro_50,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_PCA80E =  pd.merge(PCA_firm_30,PCA_macro_80,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_PCA124E =  pd.merge(PCA_firm_30,macro_all,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "PCA58F_PCA10E =  pd.merge(firm_58,PCA_macro_10,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA58F_PCA50E =  pd.merge(firm_58,PCA_macro_50,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA58F_PCA80E =  pd.merge(firm_58,PCA_macro_80,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA58F_PCA124E =  pd.merge(firm_58,macro_all,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "PCA10F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA10F_PCA10E.csv',index = False)\n",
    "PCA10F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA10F_PCA50E.csv',index = False)\n",
    "PCA10F_PCA80E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA10F_PCA80E.csv',index = False)\n",
    "PCA10F_PCA124E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA10F_PCA124E.csv',index = False)\n",
    "\n",
    "PCA30F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA30F_PCA10E.csv',index = False)\n",
    "PCA30F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA30F_PCA50E.csv',index = False)\n",
    "PCA30F_PCA80E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA30F_PCA80E.csv',index = False)\n",
    "PCA30F_PCA124E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA30F_PCA124E.csv',index = False)\n",
    "\n",
    "PCA58F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA58F_PCA10E.csv',index = False)\n",
    "PCA58F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA58F_PCA50E.csv',index = False)\n",
    "PCA58F_PCA80E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA58F_PCA80E.csv',index = False)\n",
    "PCA58F_PCA124E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/PCA58F_PCA124E.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: PCA10F_PCA10E\n",
      "Loaded: PCA10F_PCA50E\n",
      "Loaded: PCA10F_PCA80E\n",
      "Loaded: PCA10F_PCA124E\n",
      "Loaded: PCA30F_PCA10E\n",
      "Loaded: PCA30F_PCA50E\n",
      "Loaded: PCA30F_PCA80E\n",
      "Loaded: PCA30F_PCA124E\n",
      "Loaded: PCA58F_PCA10E\n",
      "Loaded: PCA58F_PCA50E\n",
      "Loaded: PCA58F_PCA80E\n",
      "Loaded: PCA58F_PCA124E\n",
      "Processing: PCA10F_PCA10E\n",
      "Processing: PCA10F_PCA50E\n",
      "Processing: PCA10F_PCA80E\n",
      "Processing: PCA10F_PCA124E\n",
      "Processing: PCA30F_PCA10E\n",
      "Processing: PCA30F_PCA50E\n",
      "Processing: PCA30F_PCA80E\n",
      "Processing: PCA30F_PCA124E\n",
      "Processing: PCA58F_PCA10E\n",
      "Processing: PCA58F_PCA50E\n",
      "Processing: PCA58F_PCA80E\n",
      "Processing: PCA58F_PCA124E\n",
      "Final Results:\n",
      "           Dataset  Average Adjusted R-squared\n",
      "0    PCA10F_PCA10E                   -0.256106\n",
      "1    PCA10F_PCA50E                   -0.316664\n",
      "2    PCA10F_PCA80E                   -0.287644\n",
      "3   PCA10F_PCA124E                   -0.565183\n",
      "4    PCA30F_PCA10E                   -0.212264\n",
      "5    PCA30F_PCA50E                   -0.387058\n",
      "6    PCA30F_PCA80E                   -0.297947\n",
      "7   PCA30F_PCA124E                   -0.607387\n",
      "8    PCA58F_PCA10E                   -0.214083\n",
      "9    PCA58F_PCA50E                   -0.293475\n",
      "10   PCA58F_PCA80E                   -0.283864\n",
      "11  PCA58F_PCA124E                   -0.465158\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_new.csv')\n",
    "y = firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "\"PCA10F_PCA10E\",\"PCA10F_PCA50E\",\"PCA10F_PCA80E\",\"PCA10F_PCA124E\",\"PCA30F_PCA10E\",\"PCA30F_PCA50E\",\"PCA30F_PCA80E\",\"PCA30F_PCA124E\",\n",
    "\"PCA58F_PCA10E\",\"PCA58F_PCA50E\",\"PCA58F_PCA80E\",\"PCA58F_PCA124E\"]\n",
    "\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "\"PCA10F_PCA10E\",\"PCA10F_PCA50E\",\"PCA10F_PCA80E\",\"PCA10F_PCA124E\",\"PCA30F_PCA10E\",\"PCA30F_PCA50E\",\"PCA30F_PCA80E\",\"PCA30F_PCA124E\",\n",
    "\"PCA58F_PCA10E\",\"PCA58F_PCA50E\",\"PCA58F_PCA80E\",\"PCA58F_PCA124E\"]\n",
    "\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (Adjusted R-squared)\n",
    "        r_squared = r2_score(validation_y, y_pred)\n",
    "        n = len(validation_y)  # Number of samples\n",
    "        k = train_X.shape[1]  # Number of predictors\n",
    "        adjusted_r2 = 1 - ((1 - r_squared) * (n - 1)) / (n - k - 1)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(adjusted_r2)\n",
    "\n",
    "    # 각 데이터셋의 평균 Adjusted R-squared 계산\n",
    "    avg_adjusted_r2 = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average Adjusted R-squared': avg_adjusted_r2})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Dataset  Average Adjusted R-squared\n",
      "0    PCA10F_PCA10E                   -0.256106\n",
      "1    PCA10F_PCA50E                   -0.316664\n",
      "2    PCA10F_PCA80E                   -0.287644\n",
      "3   PCA10F_PCA124E                   -0.565183\n",
      "4    PCA30F_PCA10E                   -0.212264\n",
      "5    PCA30F_PCA50E                   -0.387058\n",
      "6    PCA30F_PCA80E                   -0.297947\n",
      "7   PCA30F_PCA124E                   -0.607387\n",
      "8    PCA58F_PCA10E                   -0.214083\n",
      "9    PCA58F_PCA50E                   -0.293475\n",
      "10   PCA58F_PCA80E                   -0.283864\n",
      "11  PCA58F_PCA124E                   -0.465158\n"
     ]
    }
   ],
   "source": [
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "firm_s = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/scaled_firm_new.csv')\n",
    "macro_s = pd.read_csv(\"C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/scaled_macroeconomic_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_2s = firm_s.drop(['ret','month','year','ticker'],axis=1)\n",
    "macro_2s = macro_s.drop(['month','year'],axis=1)\n",
    "firm_60s = firm_s.drop(['ret','ticker'],axis = 1)\n",
    "macro_124s = macro_s\n",
    "\n",
    "\n",
    "#PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_com = 10\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(firm_2s)\n",
    "PCA_firm_10s = pd.DataFrame(pca_array, index = firm_2s.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 30\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(firm_2s)\n",
    "PCA_firm_30s = pd.DataFrame(pca_array, index = firm_2s.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 10\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(macro_2s)\n",
    "PCA_macro_10s = pd.DataFrame(pca_array, index = macro_2s.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 50\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(macro_2s)\n",
    "PCA_macro_50s = pd.DataFrame(pca_array, index = macro_2s.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])\n",
    "\n",
    "\n",
    "n_com = 80\n",
    "pca = PCA(n_components = n_com)\n",
    "pca_array = pca.fit_transform(macro_2s)\n",
    "PCA_macro_80s = pd.DataFrame(pca_array, index = macro_2s.index,\n",
    "                      columns = [f\"pca{num+1}\" for num in range(n_com)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_firm_10s['month'] = firm_60s['month']\n",
    "PCA_firm_10s['year'] = firm_60s['year']\n",
    "PCA_firm_30s['month'] = firm_60s['month']\n",
    "PCA_firm_30s['year'] = firm_60s['year']\n",
    "\n",
    "PCA_macro_10s['month'] = macro_124s['month']\n",
    "PCA_macro_50s['month'] = macro_124s['month']\n",
    "PCA_macro_80s['month'] = macro_124s['month']\n",
    "\n",
    "PCA_macro_10s['year'] = macro_124s['year']\n",
    "PCA_macro_50s['year'] = macro_124s['year']\n",
    "PCA_macro_80s['year'] = macro_124s['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_PCA10F_PCA10E =  pd.merge(PCA_firm_10s,PCA_macro_10s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA10F_PCA50E =  pd.merge(PCA_firm_10s,PCA_macro_50s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA10F_PCA80E =  pd.merge(PCA_firm_10s,PCA_macro_80s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA10F_PCA124E =  pd.merge(PCA_firm_10s,macro_124s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "S_PCA30F_PCA10E =  pd.merge(PCA_firm_30s,PCA_macro_10s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA30F_PCA50E =  pd.merge(PCA_firm_30s,PCA_macro_50s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA30F_PCA80E =  pd.merge(PCA_firm_30s,PCA_macro_80s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA30F_PCA124E =  pd.merge(PCA_firm_30s,macro_124s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "S_PCA58F_PCA10E =  pd.merge(firm_60s,PCA_macro_10s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA58F_PCA50E =  pd.merge(firm_60s,PCA_macro_50s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA58F_PCA80E =  pd.merge(firm_60s,PCA_macro_80s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "S_PCA58F_PCA124E =  pd.merge(firm_60s,macro_124s,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "S_PCA10F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA10F_PCA10E.csv',index = False)\n",
    "S_PCA10F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA10F_PCA50E.csv',index = False)\n",
    "S_PCA10F_PCA80E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA10F_PCA80E.csv',index = False)\n",
    "S_PCA10F_PCA124E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA10F_PCA124E.csv',index = False)\n",
    "\n",
    "S_PCA30F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA30F_PCA10E.csv',index = False)\n",
    "S_PCA30F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA30F_PCA50E.csv',index = False)\n",
    "S_PCA30F_PCA80E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA30F_PCA80E.csv',index = False)\n",
    "S_PCA30F_PCA124E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA30F_PCA124E.csv',index = False)\n",
    "\n",
    "S_PCA58F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA58F_PCA10E.csv',index = False)\n",
    "S_PCA58F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA58F_PCA50E.csv',index = False)\n",
    "S_PCA58F_PCA80E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA58F_PCA80E.csv',index = False)\n",
    "S_PCA58F_PCA124E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/S_PCA58F_PCA124E.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_new.csv')\n",
    "y = firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "\"S_PCA10F_PCA10E\",\"S_PCA10F_PCA50E\",\"S_PCA10F_PCA80E\",\"S_PCA10F_PCA124E\",\"S_PCA30F_PCA10E\",\"S_PCA30F_PCA50E\",\"S_PCA30F_PCA80E\",\"S_PCA30F_PCA124E\",\n",
    "\"S_PCA58F_PCA10E\",\"S_PCA58F_PCA50E\",\"S_PCA58F_PCA80E\",\"S_PCA58F_PCA124E\"]\n",
    "\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "\"S_PCA10F_PCA10E\",\"S_PCA10F_PCA50E\",\"S_PCA10F_PCA80E\",\"S_PCA10F_PCA124E\",\"S_PCA30F_PCA10E\",\"S_PCA30F_PCA50E\",\"S_PCA30F_PCA80E\",\"S_PCA30F_PCA124E\",\n",
    "\"S_PCA58F_PCA10E\",\"S_PCA58F_PCA50E\",\"S_PCA58F_PCA80E\",\"S_PCA58F_PCA124E\"]\n",
    "\n",
    "\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month','ret'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (Adjusted R-squared)\n",
    "        r_squared = r2_score(validation_y, y_pred)\n",
    "        n = len(validation_y)  # Number of samples\n",
    "        k = train_X.shape[1]  # Number of predictors\n",
    "        adjusted_r2 = 1 - ((1 - r_squared) * (n - 1)) / (n - k - 1)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(adjusted_r2)\n",
    "\n",
    "    # 각 데이터셋의 평균 Adjusted R-squared 계산\n",
    "    avg_adjusted_r2 = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average Adjusted R-squared': avg_adjusted_r2})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Dataset  Average Adjusted R-squared\n",
      "0    S_PCA10F_PCA10E                   -0.475696\n",
      "1    S_PCA10F_PCA50E                   -0.420466\n",
      "2    S_PCA10F_PCA80E                   -0.357916\n",
      "3   S_PCA10F_PCA124E                   -0.679557\n",
      "4    S_PCA30F_PCA10E                   -0.423755\n",
      "5    S_PCA30F_PCA50E                   -0.417373\n",
      "6    S_PCA30F_PCA80E                   -0.373757\n",
      "7   S_PCA30F_PCA124E                   -0.568702\n",
      "8    S_PCA58F_PCA10E                   -0.588597\n",
      "9    S_PCA58F_PCA50E                   -0.466783\n",
      "10   S_PCA58F_PCA80E                   -0.347649\n",
      "11  S_PCA58F_PCA124E                   -0.601810\n"
     ]
    }
   ],
   "source": [
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Using cached catboost-1.2.7.tar.gz (71.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [145 lines of output]\n",
      "      Collecting setuptools>=64.0\n",
      "        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "      Collecting wheel\n",
      "        Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "      Collecting jupyterlab==3.*,>=3.0.6\n",
      "        Using cached jupyterlab-3.6.8-py3-none-any.whl.metadata (12 kB)\n",
      "      Collecting conan~=1.62\n",
      "        Using cached conan-1.66.0.tar.gz (789 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting ipython (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached ipython-8.30.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "      Collecting packaging (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "      Collecting tornado>=6.1.0 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached tornado-6.4.2-cp38-abi3-win_amd64.whl.metadata (2.6 kB)\n",
      "      Collecting jupyter-core (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "      Collecting jupyterlab-server~=2.19 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "      Collecting jupyter-server<3,>=1.16.0 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "      Collecting jupyter-ydoc~=0.2.4 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_ydoc-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "      Collecting jupyter-server-ydoc~=0.8.0 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_server_ydoc-0.8.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "      Collecting nbclassic (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached nbclassic-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "      Collecting notebook<7 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached notebook-6.5.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "      Collecting jinja2>=2.1 (from jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "      Collecting requests<3.0.0,>=2.25 (from conan~=1.62)\n",
      "        Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "      Collecting urllib3<1.27,>=1.26.6 (from conan~=1.62)\n",
      "        Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "      Collecting colorama<0.5.0,>=0.3.3 (from conan~=1.62)\n",
      "        Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "      Collecting PyYAML<6.1,>=3.11 (from conan~=1.62)\n",
      "        Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "      Collecting patch-ng<1.18,>=1.17.4 (from conan~=1.62)\n",
      "        Using cached patch-ng-1.17.4.tar.gz (17 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting fasteners>=0.14.1 (from conan~=1.62)\n",
      "        Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "      Collecting six<=1.16.0,>=1.10.0 (from conan~=1.62)\n",
      "        Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "      Collecting node-semver==0.6.1 (from conan~=1.62)\n",
      "        Using cached node_semver-0.6.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "      Collecting pygments<3.0,>=2.0 (from conan~=1.62)\n",
      "        Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "      Collecting tqdm<5,>=4.28.1 (from conan~=1.62)\n",
      "        Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "      Collecting python-dateutil<3,>=2.7.0 (from conan~=1.62)\n",
      "        Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "      Collecting bottle<0.13,>=0.12.8 (from conan~=1.62)\n",
      "        Using cached bottle-0.12.25-py3-none-any.whl.metadata (1.8 kB)\n",
      "      Collecting pluginbase>=0.5 (from conan~=1.62)\n",
      "        Using cached pluginbase-1.0.1.tar.gz (43 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting PyJWT<3.0.0,>=2.4.0 (from conan~=1.62)\n",
      "        Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "      Collecting MarkupSafe>=2.0 (from jinja2>=2.1->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "      Collecting anyio>=3.1.0 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "      Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "      Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "      Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "      Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "      Collecting nbconvert>=6.4.4 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached nbconvert-7.16.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "      Collecting nbformat>=5.3.0 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "      Collecting overrides>=5.0 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "      Collecting prometheus-client>=0.9 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "      Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached pywinpty-2.0.14-cp313-none-win_amd64.whl.metadata (5.2 kB)\n",
      "      Collecting pyzmq>=24 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached pyzmq-26.2.0-cp313-cp313-win_amd64.whl.metadata (6.2 kB)\n",
      "      Collecting send2trash>=1.8.2 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "      Collecting terminado>=0.8.3 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "      Collecting traitlets>=5.6.0 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "      Collecting websocket-client>=1.7 (from jupyter-server<3,>=1.16.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "      Collecting platformdirs>=2.5 (from jupyter-core->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "      Collecting pywin32>=300 (from jupyter-core->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached pywin32-308-cp313-cp313-win_amd64.whl.metadata (8.3 kB)\n",
      "      Collecting jupyter-server-fileid<1,>=0.6.0 (from jupyter-server-ydoc~=0.8.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached jupyter_server_fileid-0.9.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "      Collecting ypy-websocket<0.9.0,>=0.8.2 (from jupyter-server-ydoc~=0.8.0->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached ypy_websocket-0.8.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "      Collecting y-py<0.7.0,>=0.6.0 (from jupyter-ydoc~=0.2.4->jupyterlab==3.*,>=3.0.6)\n",
      "        Using cached y_py-0.6.2.tar.gz (53 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        횞 Preparing metadata (pyproject.toml) did not run successfully.\n",
      "        \\xe2봻 exit code: 1\n",
      "        \\xe2빊\\xe2\\x94\\x80> [6 lines of output]\n",
      "            Checking for Rust toolchain....\n",
      "      \n",
      "            Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "            This package requires Rust and Cargo to compile extensions. Install it through\n",
      "            the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: metadata-generation-failed\n",
      "      \n",
      "      횞 Encountered error while generating package metadata.\n",
      "      \\xe2빊\\xe2\\x94\\x80> See above for output.\n",
      "      \n",
      "      note: This is an issue with the package mentioned above, not pip.\n",
      "      hint: See above for details.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dabin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.3.1)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "Successfully installed setuptools-75.6.0 wheel-0.45.1\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[0;32m      7\u001b[0m firm \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_new.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m firm[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mret\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/firm_new.csv')\n",
    "y = firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/최종데이터/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "\"S_PCA10F_PCA10E\",\"S_PCA10F_PCA50E\",\"S_PCA10F_PCA80E\",\"S_PCA10F_PCA124E\",\"S_PCA30F_PCA10E\",\"S_PCA30F_PCA50E\",\"S_PCA30F_PCA80E\",\"S_PCA30F_PCA124E\",\n",
    "\"S_PCA58F_PCA10E\",\"S_PCA58F_PCA50E\",\"S_PCA58F_PCA80E\",\"S_PCA58F_PCA124E\"]\n",
    "\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "\"S_PCA10F_PCA10E\",\"S_PCA10F_PCA50E\",\"S_PCA10F_PCA80E\",\"S_PCA10F_PCA124E\",\"S_PCA30F_PCA10E\",\"S_PCA30F_PCA50E\",\"S_PCA30F_PCA80E\",\"S_PCA30F_PCA124E\",\n",
    "\"S_PCA58F_PCA10E\",\"S_PCA58F_PCA50E\",\"S_PCA58F_PCA80E\",\"S_PCA58F_PCA124E\"]\n",
    "\n",
    "\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month','ret'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=7,\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (Adjusted R-squared)\n",
    "        r_squared = r2_score(validation_y, y_pred)\n",
    "        n = len(validation_y)  # Number of samples\n",
    "        k = train_X.shape[1]  # Number of predictors\n",
    "        adjusted_r2 = 1 - ((1 - r_squared) * (n - 1)) / (n - k - 1)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(adjusted_r2)\n",
    "\n",
    "    # 각 데이터셋의 평균 Adjusted R-squared 계산\n",
    "    avg_adjusted_r2 = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average Adjusted R-squared': avg_adjusted_r2})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
