{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pca10 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터/PCA_firm_10.csv')\n",
    "pca30 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터/PCA_firm_30.csv')\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "\n",
    "\n",
    "\n",
    "ua10 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터/UMAP_firm_10.csv')\n",
    "ua30 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터/UMAP_firm_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca10['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "pca30['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "\n",
    "pca10['year'] = firm['year']\n",
    "pca30['year'] = firm['year']\n",
    "\n",
    "\n",
    "ua10['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "ua30['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "\n",
    "ua10['year'] = firm['year']\n",
    "ua30['year'] = firm['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua10['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "ua30['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "\n",
    "ua10['year'] = firm['year']\n",
    "ua30['year'] = firm['year']\n",
    "\n",
    "\n",
    "ua10 = ua10.drop(index = ua10.loc[(ua10['year'] == 1997) & (ua10['month'] == 1 ),:].index, axis = 2)\n",
    "ua10 = ua10.reset_index().drop(['index'],axis = 1)\n",
    "ua10.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_firm_10.csv',index = False)\n",
    "\n",
    "\n",
    "\n",
    "ua30 = ua30.drop(index = ua30.loc[(ua30['year'] == 1997) & (ua30['month'] == 1 ),:].index, axis = 2)\n",
    "ua30 = ua30.reset_index().drop(['index'],axis = 1)\n",
    "ua30.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_firm_30.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca10 = pca10.drop(index = pca10.loc[(pca10['year'] == 1997) & (pca10['month'] == 1 ),:].index, axis = 2)\n",
    "pca10 = pca10.reset_index().drop(['index'],axis = 1)\n",
    "pca10.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_firm_10.csv',index = False)\n",
    "\n",
    "\n",
    "\n",
    "pca30 = pca30.drop(index = pca30.loc[(pca30['year'] == 1997) & (pca30['month'] == 1 ),:].index, axis = 2)\n",
    "pca30 = pca30.reset_index().drop(['index'],axis = 1)\n",
    "pca30.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_firm_30.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA10F = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_firm_10.csv')\n",
    "PCA30F = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_firm_30.csv')\n",
    "UA10F = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_firm_10.csv')\n",
    "UA30F = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_firm_30.csv')\n",
    "AE10F = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/AE_firm_10.csv')\n",
    "AE10F = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/AE_firm_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_500 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "\n",
    "firm_500['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "firm_500['month'] = pd.to_datetime(firm['date']).dt.month\n",
    "\n",
    "firm_500['year'] = firm_500['year']\n",
    "firm_500['year'] = firm_500['year']\n",
    "\n",
    "\n",
    "\n",
    "firm_500 = firm_500.drop(index = firm_500.loc[(firm_500['year'] == 1997) & (firm_500['month'] == 1 ), :].index, axis = 2)\n",
    "firm_500 = firm_500.reset_index().drop(['index'],axis = 1)\n",
    "\n",
    "firm_500.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm58 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105667, 63)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firm58.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/macroeconomic_new.csv')\n",
    "PCA10E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_macro_10.csv')\n",
    "PCA30E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_macro_30.csv')\n",
    "PCA50E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/PCA_macro_50.csv')\n",
    "UA10E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_macro_10.csv')\n",
    "UA30E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_macro_30.csv')\n",
    "UA50E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/UMAP_macro_50.csv')\n",
    "AE10E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/AE_macro_10.csv')\n",
    "AE30E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/AE_macro_30.csv')\n",
    "AE50E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/AE_macro_50.csv')\n",
    "LT10E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/LSTM_macro_10.csv')\n",
    "LT30E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/LSTM_macro_30.csv')\n",
    "LT50E = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/LSTM_macro_50.csv')\n",
    "\n",
    "\n",
    "macro = macro.drop('sasdate',axis = 1)\n",
    "\n",
    "UA10E['month'] = macro['month']\n",
    "UA10E['year'] = macro['year']\n",
    "UA30E['month'] = macro['month']\n",
    "UA30E['year'] = macro['year']\n",
    "UA50E['month'] = macro['month']\n",
    "UA50E['year'] = macro['year']\n",
    "AE10E['month'] = macro['month']\n",
    "AE10E['year'] = macro['year']\n",
    "AE30E['month'] = macro['month']\n",
    "AE30E['year'] = macro['year']\n",
    "AE50E['month'] = macro['month']\n",
    "AE50E['year'] = macro['year']\n",
    "\n",
    "PCA10E['month'] = macro['month']\n",
    "PCA10E['year'] = macro['year']\n",
    "PCA30E['month'] = macro['month']\n",
    "PCA30E['year'] = macro['year']\n",
    "PCA50E['month'] = macro['month']\n",
    "PCA50E['year'] = macro['year']\n",
    "\n",
    "\n",
    "LT10E['month'] = macro['month']\n",
    "LT10E['year'] = macro['year']\n",
    "LT30E['month'] = macro['month']\n",
    "LT30E['year'] = macro['year']\n",
    "LT50E['month'] = macro['month']\n",
    "LT50E['year'] = macro['year']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PCA10F_UA10E= pd.merge(PCA10F, UA10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_UA30E= pd.merge(PCA10F, UA30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_UA50E= pd.merge(PCA10F, UA50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_AE10E= pd.merge(PCA10F, AE10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_AE30E= pd.merge(PCA10F, AE30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_AE50E= pd.merge(PCA10F, AE50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_PCA10E= pd.merge(PCA10F, PCA10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_PCA30E= pd.merge(PCA10F,PCA30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_PCA50E= pd.merge(PCA10F, PCA50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_LT10E= pd.merge(PCA10F, LT10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_LT30E= pd.merge(PCA10F,LT30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA10F_LT50E= pd.merge(PCA10F,LT50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "\n",
    "PCA30F_UA10E= pd.merge(PCA30F, UA10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_UA30E= pd.merge(PCA30F, UA30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_UA50E= pd.merge(PCA30F, UA50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_AE10E= pd.merge(PCA30F, AE10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_AE30E= pd.merge(PCA30F, AE30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_AE50E= pd.merge(PCA30F, AE50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_PCA10E= pd.merge(PCA30F, PCA10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_PCA30E= pd.merge(PCA30F, PCA30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_PCA50E= pd.merge(PCA30F, PCA50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_LT10E= pd.merge(PCA30F, LT10E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_LT30E= pd.merge(PCA30F, LT30E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_LT50E= pd.merge(PCA30F, LT50E,\n",
    "                     left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "\n",
    "PCA10F_58E = pd.merge(PCA10F, macro, left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n",
    "PCA30F_58E = pd.merge(PCA30F, macro, left_on= [\"month\",\"year\"], right_on= [\"month\",\"year\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA10F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_PCA10E.csv',index = False)\n",
    "PCA10F_PCA30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_PCA30E.csv',index = False)\n",
    "PCA10F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_PCA50E.csv',index = False)\n",
    "\n",
    "PCA10F_UA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_UA10E.csv',index = False)\n",
    "PCA10F_UA30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_UA30E.csv',index = False)\n",
    "PCA10F_UA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_UA50E.csv',index = False)\n",
    "\n",
    "PCA10F_AE10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_AE10E.csv',index = False)\n",
    "PCA10F_AE30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_AE30E.csv',index = False)\n",
    "PCA10F_AE50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_AE50E.csv',index = False)\n",
    "\n",
    "PCA10F_LT10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_LT10E.csv',index = False)\n",
    "PCA10F_LT30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_LT30E.csv',index = False)\n",
    "PCA10F_LT50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_LT50E.csv',index = False)\n",
    "\n",
    "PCA30F_PCA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_PCA10E.csv',index = False)\n",
    "PCA30F_PCA30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_PCA30E.csv',index = False)\n",
    "PCA30F_PCA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_PCA50E.csv',index = False)\n",
    "\n",
    "PCA30F_UA10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_UA10E.csv',index = False)\n",
    "PCA30F_UA30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_UA30E.csv',index = False)\n",
    "PCA30F_UA50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_UA50E.csv',index = False)\n",
    "\n",
    "PCA30F_AE10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_AE10E.csv',index = False)\n",
    "PCA30F_AE30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_AE30E.csv',index = False)\n",
    "PCA30F_AE50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_AE50E.csv',index = False)\n",
    "\n",
    "PCA30F_LT10E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_LT10E.csv',index = False)\n",
    "PCA30F_LT30E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_LT30E.csv',index = False)\n",
    "PCA30F_LT50E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_LT50E.csv',index = False)\n",
    "\n",
    "PCA10F_58E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA10F_58E.csv',index = False)\n",
    "PCA30F_58E.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/PCA30F_58E.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_500 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/DATA/firm_500.csv')\n",
    "firm_500 = firm_500.drop(columns=['ret','ticker','date','Founded','gvkey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: AE10F_124E\n",
      "Loaded: AE10F_AE10E\n",
      "Loaded: AE10F_AE30E\n",
      "Loaded: AE10F_AE50E\n",
      "Loaded: AE10F_LT10E\n",
      "Loaded: AE10F_LT30E\n",
      "Loaded: AE10F_LT50E\n",
      "Loaded: AE10F_PCA10E\n",
      "Loaded: AE10F_PCA30E\n",
      "Loaded: AE10F_PCA50E\n",
      "Loaded: AE10F_UA10E\n",
      "Loaded: AE10F_UA30E\n",
      "Loaded: AE10F_UA50E\n",
      "Loaded: AE30F_124E\n",
      "Loaded: AE30F_AE10E\n",
      "Loaded: AE30F_AE30E\n",
      "Loaded: AE30F_AE50E\n",
      "Loaded: AE30F_LT10E\n",
      "Loaded: AE30F_LT30E\n",
      "Loaded: AE30F_LT50E\n",
      "Loaded: AE30F_PCA10E\n",
      "Loaded: AE30F_PCA30E\n",
      "Loaded: AE30F_PCA50E\n",
      "Loaded: AE30F_UA10E\n",
      "Loaded: AE30F_UA30E\n",
      "Loaded: AE30F_UA50E\n",
      "Processing: AE10F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21667\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22872\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24072\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25260\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26449\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3312\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3348\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3384\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3444\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4985\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5117\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5256\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5384\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5498\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6557\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6840\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7140\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7447\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7738\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4264\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4382\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4499\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4614\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4733\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7734\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8080\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8430\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8779\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9131\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10894\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11447\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11998\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12554\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13110\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4350\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4470\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4710\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4830\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7950\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8310\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9030\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9390\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11550\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12150\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13350\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13950\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4349\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4468\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4706\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4826\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7950\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8310\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9030\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9381\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE10F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11550\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12150\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13350\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13943\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26767\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27972\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30360\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31549\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8412\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8448\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8484\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8518\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8544\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10085\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10217\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10356\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10484\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10598\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11657\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11940\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12240\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12547\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12838\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9364\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9482\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9599\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9714\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9833\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12834\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13180\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13530\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13879\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14231\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15994\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16547\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17098\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17654\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18210\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9450\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9810\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9930\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13050\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13410\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13770\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14130\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14490\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16650\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17250\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17850\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18450\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19050\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9449\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9568\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9688\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9806\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9926\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037625 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13050\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13410\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13770\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14130\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14481\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: AE30F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16650\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17250\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17850\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18450\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19043\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Final Results:\n",
      "         Dataset  Average MSE\n",
      "0     AE10F_124E     0.007246\n",
      "1    AE10F_AE10E     0.005820\n",
      "2    AE10F_AE30E     0.006454\n",
      "3    AE10F_AE50E     0.006773\n",
      "4    AE10F_LT10E     0.006801\n",
      "5    AE10F_LT30E     0.007935\n",
      "6    AE10F_LT50E     0.006928\n",
      "7   AE10F_PCA10E     0.006762\n",
      "8   AE10F_PCA30E     0.005923\n",
      "9   AE10F_PCA50E     0.005900\n",
      "10   AE10F_UA10E     0.005583\n",
      "11   AE10F_UA30E     0.006237\n",
      "12   AE10F_UA50E     0.007421\n",
      "13    AE30F_124E     0.007005\n",
      "14   AE30F_AE10E     0.005693\n",
      "15   AE30F_AE30E     0.006522\n",
      "16   AE30F_AE50E     0.007039\n",
      "17   AE30F_LT10E     0.006365\n",
      "18   AE30F_LT30E     0.008271\n",
      "19   AE30F_LT50E     0.006842\n",
      "20  AE30F_PCA10E     0.006642\n",
      "21  AE30F_PCA30E     0.005879\n",
      "22  AE30F_PCA50E     0.005708\n",
      "23   AE30F_UA10E     0.005597\n",
      "24   AE30F_UA30E     0.006299\n",
      "25   AE30F_UA50E     0.007041\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y = firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/Firm_AE/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "    \"AE10F_124E\", \"AE10F_AE10E\", \"AE10F_AE30E\", \"AE10F_AE50E\", \"AE10F_LT10E\",\n",
    "    \"AE10F_LT30E\", \"AE10F_LT50E\", \"AE10F_PCA10E\", \"AE10F_PCA30E\", \"AE10F_PCA50E\",\n",
    "    \"AE10F_UA10E\", \"AE10F_UA30E\", \"AE10F_UA50E\", \"AE30F_124E\", \"AE30F_AE10E\",\n",
    "    \"AE30F_AE30E\", \"AE30F_AE50E\", \"AE30F_LT10E\", \"AE30F_LT30E\", \"AE30F_LT50E\",\n",
    "    \"AE30F_PCA10E\", \"AE30F_PCA30E\", \"AE30F_PCA50E\", \"AE30F_UA10E\", \"AE30F_UA30E\",\n",
    "    \"AE30F_UA50E\"\n",
    "]\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "    \"AE10F_124E\", \"AE10F_AE10E\", \"AE10F_AE30E\", \"AE10F_AE50E\",\n",
    "    \"AE10F_LT10E\", \"AE10F_LT30E\", \"AE10F_LT50E\", \"AE10F_PCA10E\",\n",
    "    \"AE10F_PCA30E\", \"AE10F_PCA50E\", \"AE10F_UA10E\", \"AE10F_UA30E\",\n",
    "    \"AE10F_UA50E\", \"AE30F_124E\", \"AE30F_AE10E\", \"AE30F_AE30E\",\n",
    "    \"AE30F_AE50E\", \"AE30F_LT10E\", \"AE30F_LT30E\", \"AE30F_LT50E\",\n",
    "    \"AE30F_PCA10E\", \"AE30F_PCA30E\", \"AE30F_PCA50E\", \"AE30F_UA10E\",\n",
    "    \"AE30F_UA30E\", \"AE30F_UA50E\"\n",
    "]\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = lgb.LGBMRegressor(random_state = 42)\n",
    "        train_X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        # 데이터프레임 컬럼 이름\n",
    "        columns = train_X.columns\n",
    "\n",
    "        # 허용되지 않는 특수 문자 패턴 정의\n",
    "        invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "        # 문제 있는 컬럼 이름 탐지\n",
    "        invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "        train_X.columns = train_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "        validation_X.columns = validation_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (MSE)\n",
    "        mse = mean_squared_error(validation_y, y_pred)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(mse)\n",
    "\n",
    "    # 각 데이터셋의 평균 MSE 계산\n",
    "    avg_mse = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average MSE': avg_mse})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)\n",
    "\n",
    "# 결과를 CSV로 저장\n",
    "final_results_df.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/AE_average_mse_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: PCA10F_124E\n",
      "Loaded: PCA10F_AE10E\n",
      "Loaded: PCA10F_AE30E\n",
      "Loaded: PCA10F_AE50E\n",
      "Loaded: PCA10F_LT10E\n",
      "Loaded: PCA10F_LT30E\n",
      "Loaded: PCA10F_LT50E\n",
      "Loaded: PCA10F_PCA10E\n",
      "Loaded: PCA10F_PCA30E\n",
      "Loaded: PCA10F_PCA50E\n",
      "Loaded: PCA10F_UA10E\n",
      "Loaded: PCA10F_UA30E\n",
      "Loaded: PCA10F_UA50E\n",
      "Loaded: PCA30F_124E\n",
      "Loaded: PCA30F_AE10E\n",
      "Loaded: PCA30F_AE30E\n",
      "Loaded: PCA30F_AE50E\n",
      "Loaded: PCA30F_LT10E\n",
      "Loaded: PCA30F_LT30E\n",
      "Loaded: PCA30F_LT50E\n",
      "Loaded: PCA30F_PCA10E\n",
      "Loaded: PCA30F_PCA30E\n",
      "Loaded: PCA30F_PCA50E\n",
      "Loaded: PCA30F_UA10E\n",
      "Loaded: PCA30F_UA30E\n",
      "Loaded: PCA30F_UA50E\n",
      "Processing: PCA10F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21667\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22872\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24072\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25260\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26449\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3312\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3348\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3384\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3444\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4985\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5117\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5256\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5384\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5498\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6557\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6840\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7140\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7447\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7738\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4264\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4382\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4499\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4614\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4733\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7734\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8080\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8430\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8779\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9131\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10894\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11447\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11998\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12554\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13110\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4350\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4470\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4590\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4710\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4830\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7950\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8310\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9030\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9390\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11550\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12150\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13350\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13950\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4349\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4468\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4588\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4706\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4826\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7950\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8310\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9030\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9381\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA10F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11550\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12150\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13350\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13943\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26767\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27972\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29172\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30360\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31549\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8412\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8448\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8484\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8518\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8544\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10085\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10217\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10356\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10484\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10598\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11657\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11940\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12240\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12547\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12838\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9364\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9482\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9599\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9714\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9833\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12834\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13180\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13530\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13879\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14231\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15994\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16547\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17098\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17654\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18210\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9450\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9570\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9690\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9810\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9930\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13050\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13410\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13770\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14130\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14490\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16650\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17250\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17850\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18450\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19050\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9449\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9568\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9688\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9806\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9926\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13050\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13410\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13770\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14130\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14481\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: PCA30F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16650\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17250\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17850\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18450\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19043\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Final Results:\n",
      "          Dataset  Average MSE\n",
      "0     PCA10F_124E     0.007272\n",
      "1    PCA10F_AE10E     0.005775\n",
      "2    PCA10F_AE30E     0.006313\n",
      "3    PCA10F_AE50E     0.007474\n",
      "4    PCA10F_LT10E     0.006456\n",
      "5    PCA10F_LT30E     0.007995\n",
      "6    PCA10F_LT50E     0.006537\n",
      "7   PCA10F_PCA10E     0.006737\n",
      "8   PCA10F_PCA30E     0.005930\n",
      "9   PCA10F_PCA50E     0.005780\n",
      "10   PCA10F_UA10E     0.005646\n",
      "11   PCA10F_UA30E     0.006075\n",
      "12   PCA10F_UA50E     0.007294\n",
      "13    PCA30F_124E     0.007583\n",
      "14   PCA30F_AE10E     0.005931\n",
      "15   PCA30F_AE30E     0.006376\n",
      "16   PCA30F_AE50E     0.006947\n",
      "17   PCA30F_LT10E     0.006522\n",
      "18   PCA30F_LT30E     0.007415\n",
      "19   PCA30F_LT50E     0.006604\n",
      "20  PCA30F_PCA10E     0.005994\n",
      "21  PCA30F_PCA30E     0.005833\n",
      "22  PCA30F_PCA50E     0.005702\n",
      "23   PCA30F_UA10E     0.005560\n",
      "24   PCA30F_UA30E     0.006212\n",
      "25   PCA30F_UA50E     0.006047\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "firm = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y = firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "    \"PCA10F_124E\", \"PCA10F_AE10E\", \"PCA10F_AE30E\", \"PCA10F_AE50E\", \"PCA10F_LT10E\",\n",
    "    \"PCA10F_LT30E\", \"PCA10F_LT50E\", \"PCA10F_PCA10E\", \"PCA10F_PCA30E\", \"PCA10F_PCA50E\",\n",
    "    \"PCA10F_UA10E\", \"PCA10F_UA30E\", \"PCA10F_UA50E\", \"PCA30F_124E\", \"PCA30F_AE10E\",\n",
    "    \"PCA30F_AE30E\", \"PCA30F_AE50E\", \"PCA30F_LT10E\", \"PCA30F_LT30E\", \"PCA30F_LT50E\",\n",
    "    \"PCA30F_PCA10E\", \"PCA30F_PCA30E\", \"PCA30F_PCA50E\", \"PCA30F_UA10E\", \"PCA30F_UA30E\",\n",
    "    \"PCA30F_UA50E\"\n",
    "]\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "    \"PCA10F_124E\", \"PCA10F_AE10E\", \"PCA10F_AE30E\", \"PCA10F_AE50E\", \"PCA10F_LT10E\",\n",
    "    \"PCA10F_LT30E\", \"PCA10F_LT50E\", \"PCA10F_PCA10E\", \"PCA10F_PCA30E\", \"PCA10F_PCA50E\",\n",
    "    \"PCA10F_UA10E\", \"PCA10F_UA30E\", \"PCA10F_UA50E\", \"PCA30F_124E\", \"PCA30F_AE10E\",\n",
    "    \"PCA30F_AE30E\", \"PCA30F_AE50E\", \"PCA30F_LT10E\", \"PCA30F_LT30E\", \"PCA30F_LT50E\",\n",
    "    \"PCA30F_PCA10E\", \"PCA30F_PCA30E\", \"PCA30F_PCA50E\", \"PCA30F_UA10E\", \"PCA30F_UA30E\",\n",
    "    \"PCA30F_UA50E\"\n",
    "]\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = lgb.LGBMRegressor(random_state = 42)\n",
    "        train_X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        # 데이터프레임 컬럼 이름\n",
    "        columns = train_X.columns\n",
    "\n",
    "        # 허용되지 않는 특수 문자 패턴 정의\n",
    "        invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "        # 문제 있는 컬럼 이름 탐지\n",
    "        invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "        train_X.columns = train_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "        validation_X.columns = validation_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (MSE)\n",
    "        mse = mean_squared_error(validation_y, y_pred)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(mse)\n",
    "\n",
    "    # 각 데이터셋의 평균 MSE 계산\n",
    "    avg_mse = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average MSE': avg_mse})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)\n",
    "\n",
    "# 결과를 CSV로 저장\n",
    "final_results_df.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/PCA_average_mse_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: UA10F_124E\n",
      "Loaded: UA10F_AE10E\n",
      "Loaded: UA10F_AE30E\n",
      "Loaded: UA10F_AE50E\n",
      "Loaded: UA10F_LT10E\n",
      "Loaded: UA10F_LT30E\n",
      "Loaded: UA10F_LT50E\n",
      "Loaded: UA10F_PCA10E\n",
      "Loaded: UA10F_PCA30E\n",
      "Loaded: UA10F_PCA50E\n",
      "Loaded: UA10F_UA10E\n",
      "Loaded: UA10F_UA30E\n",
      "Loaded: UA10F_UA50E\n",
      "Loaded: UA30F_124E\n",
      "Loaded: UA30F_AE10E\n",
      "Loaded: UA30F_AE30E\n",
      "Loaded: UA30F_AE50E\n",
      "Loaded: UA30F_LT10E\n",
      "Loaded: UA30F_LT30E\n",
      "Loaded: UA30F_LT50E\n",
      "Loaded: UA30F_PCA10E\n",
      "Loaded: UA30F_PCA30E\n",
      "Loaded: UA30F_PCA50E\n",
      "Loaded: UA30F_UA10E\n",
      "Loaded: UA30F_UA30E\n",
      "Loaded: UA30F_UA50E\n",
      "Processing: UA10F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21922\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23127\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24327\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25515\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26704\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3567\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3603\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3639\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3673\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3699\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5240\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5372\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5511\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5639\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5753\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6812\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7095\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7702\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7993\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4519\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4637\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4754\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4869\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4988\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7989\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8335\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8685\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9034\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9386\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11149\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11702\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12253\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12809\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13365\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4605\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4725\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4965\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5085\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8205\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8565\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8925\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9285\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9645\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11805\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12405\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13005\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13605\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14205\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4604\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4723\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4843\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4961\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5081\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8205\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8565\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8925\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9285\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9636\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA10F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11805\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12405\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13005\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13605\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14198\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27022\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 153\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28227\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 153\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29427\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 153\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30615\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 153\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31804\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 153\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8667\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8703\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8739\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8773\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8799\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10340\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10472\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10611\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10739\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10853\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11912\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12195\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12495\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12802\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13093\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9619\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9737\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9854\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9969\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10088\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13089\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13435\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13785\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14134\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14486\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16249\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16802\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17353\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17909\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18465\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9705\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9825\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9945\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10065\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10185\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13305\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13665\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14025\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14385\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14745\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16905\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17505\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18105\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18705\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19305\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9704\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9823\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9943\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10061\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10181\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13305\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13665\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14025\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14385\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14736\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: UA30F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16905\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17505\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18105\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18705\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19298\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Final Results:\n",
      "         Dataset  Average MSE\n",
      "0     UA10F_124E     0.006647\n",
      "1    UA10F_AE10E     0.005740\n",
      "2    UA10F_AE30E     0.006312\n",
      "3    UA10F_AE50E     0.006980\n",
      "4    UA10F_LT10E     0.006258\n",
      "5    UA10F_LT30E     0.007827\n",
      "6    UA10F_LT50E     0.006272\n",
      "7   UA10F_PCA10E     0.006897\n",
      "8   UA10F_PCA30E     0.005812\n",
      "9   UA10F_PCA50E     0.005749\n",
      "10   UA10F_UA10E     0.005762\n",
      "11   UA10F_UA30E     0.006039\n",
      "12   UA10F_UA50E     0.007329\n",
      "13    UA30F_124E     0.006916\n",
      "14   UA30F_AE10E     0.005729\n",
      "15   UA30F_AE30E     0.006242\n",
      "16   UA30F_AE50E     0.006705\n",
      "17   UA30F_LT10E     0.006490\n",
      "18   UA30F_LT30E     0.007751\n",
      "19   UA30F_LT50E     0.006668\n",
      "20  UA30F_PCA10E     0.006848\n",
      "21  UA30F_PCA30E     0.005804\n",
      "22  UA30F_PCA50E     0.005693\n",
      "23   UA30F_UA10E     0.005744\n",
      "24   UA30F_UA30E     0.006064\n",
      "25   UA30F_UA50E     0.007472\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/Firm_UMAP/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "    \"UA10F_124E\", \"UA10F_AE10E\", \"UA10F_AE30E\", \"UA10F_AE50E\", \"UA10F_LT10E\",\n",
    "    \"UA10F_LT30E\", \"UA10F_LT50E\", \"UA10F_PCA10E\", \"UA10F_PCA30E\", \"UA10F_PCA50E\",\n",
    "    \"UA10F_UA10E\", \"UA10F_UA30E\", \"UA10F_UA50E\", \"UA30F_124E\", \"UA30F_AE10E\",\n",
    "    \"UA30F_AE30E\", \"UA30F_AE50E\", \"UA30F_LT10E\", \"UA30F_LT30E\", \"UA30F_LT50E\",\n",
    "    \"UA30F_PCA10E\", \"UA30F_PCA30E\", \"UA30F_PCA50E\", \"UA30F_UA10E\", \"UA30F_UA30E\",\n",
    "    \"UA30F_UA50E\"\n",
    "]\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "    \"UA10F_124E\", \"UA10F_AE10E\", \"UA10F_AE30E\", \"UA10F_AE50E\", \"UA10F_LT10E\",\n",
    "    \"UA10F_LT30E\", \"UA10F_LT50E\", \"UA10F_PCA10E\", \"UA10F_PCA30E\", \"UA10F_PCA50E\",\n",
    "    \"UA10F_UA10E\", \"UA10F_UA30E\", \"UA10F_UA50E\", \"UA30F_124E\", \"UA30F_AE10E\",\n",
    "    \"UA30F_AE30E\", \"UA30F_AE50E\", \"UA30F_LT10E\", \"UA30F_LT30E\", \"UA30F_LT50E\",\n",
    "    \"UA30F_PCA10E\", \"UA30F_PCA30E\", \"UA30F_PCA50E\", \"UA30F_UA10E\", \"UA30F_UA30E\",\n",
    "    \"UA30F_UA50E\"\n",
    "]\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = lgb.LGBMRegressor(random_state = 42)\n",
    "        train_X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        # 데이터프레임 컬럼 이름\n",
    "        columns = train_X.columns\n",
    "\n",
    "        # 허용되지 않는 특수 문자 패턴 정의\n",
    "        invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "        # 문제 있는 컬럼 이름 탐지\n",
    "        invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "        train_X.columns = train_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "        validation_X.columns = validation_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (MSE)\n",
    "        mse = mean_squared_error(validation_y, y_pred)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(mse)\n",
    "\n",
    "    # 각 데이터셋의 평균 MSE 계산\n",
    "    avg_mse = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average MSE': avg_mse})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)\n",
    "\n",
    "# 결과를 CSV로 저장\n",
    "final_results_df.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/UA_average_mse_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 58F_124E\n",
      "Loaded: 58F_AE10E\n",
      "Loaded: 58F_AE30E\n",
      "Loaded: 58F_AE50E\n",
      "Loaded: 58F_LT10E\n",
      "Loaded: 58F_LT30E\n",
      "Loaded: 58F_LT50E\n",
      "Loaded: 58F_PCA10E\n",
      "Loaded: 58F_PCA30E\n",
      "Loaded: 58F_PCA50E\n",
      "Loaded: 58F_UA10E\n",
      "Loaded: 58F_UA30E\n",
      "Loaded: 58F_UA50E\n",
      "Processing: 58F_124E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32125\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33329\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34530\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35720\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36910\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_AE10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13770\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13805\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13842\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13878\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13905\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_AE30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15443\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15574\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15714\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15844\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15959\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_AE50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17015\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 103\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17297\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 103\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17598\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 103\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17907\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 103\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18199\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 103\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_LT10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14722\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14839\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14957\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15074\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15194\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_LT30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18192\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18537\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18888\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19239\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19592\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_LT50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21352\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21904\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22456\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23014\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23571\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_PCA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14808\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14927\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15048\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15170\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15291\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_PCA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18408\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18767\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19128\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19490\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19851\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_PCA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22008\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22607\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 23208\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23810\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24411\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_UA10E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14807\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14925\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15046\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15166\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15287\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_UA30E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18408\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18767\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19128\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19490\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19842\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Processing: 58F_UA50E\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22008\n",
      "[LightGBM] [Info] Number of data points in the train set: 56254, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.013326\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22607\n",
      "[LightGBM] [Info] Number of data points in the train set: 60701, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.013591\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23208\n",
      "[LightGBM] [Info] Number of data points in the train set: 65223, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014708\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23810\n",
      "[LightGBM] [Info] Number of data points in the train set: 69823, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014702\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24404\n",
      "[LightGBM] [Info] Number of data points in the train set: 74519, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 0.014011\n",
      "Final Results:\n",
      "       Dataset  Average MSE\n",
      "0     58F_124E     0.006881\n",
      "1    58F_AE10E     0.005783\n",
      "2    58F_AE30E     0.006464\n",
      "3    58F_AE50E     0.007218\n",
      "4    58F_LT10E     0.006394\n",
      "5    58F_LT30E     0.008121\n",
      "6    58F_LT50E     0.007028\n",
      "7   58F_PCA10E     0.006543\n",
      "8   58F_PCA30E     0.005917\n",
      "9   58F_PCA50E     0.005735\n",
      "10   58F_UA10E     0.005683\n",
      "11   58F_UA30E     0.006071\n",
      "12   58F_UA50E     0.006622\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/병합데이터/Firm_58/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "    \"58F_124E\", \"58F_AE10E\", \"58F_AE30E\", \"58F_AE50E\", \"58F_LT10E\",\n",
    "    \"58F_LT30E\", \"58F_LT50E\", \"58F_PCA10E\", \"58F_PCA30E\", \"58F_PCA50E\",\n",
    "    \"58F_UA10E\", \"58F_UA30E\", \"58F_UA50E\"]\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets = [\n",
    "    \"58F_124E\", \"58F_AE10E\", \"58F_AE30E\", \"58F_AE50E\", \"58F_LT10E\",\n",
    "    \"58F_LT30E\", \"58F_LT50E\", \"58F_PCA10E\", \"58F_PCA30E\", \"58F_PCA50E\",\n",
    "    \"58F_UA10E\", \"58F_UA30E\", \"58F_UA50E\"]\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "    # Validation 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # Validation 연도별 반복 (2012년부터 2016년까지)\n",
    "    for validation_year in range(2012, 2017):\n",
    "        # 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "        train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= validation_year - 1)].drop(columns=['year', 'month'])\n",
    "        train_y = y_data[(y_data['year'] >= 1997) & (y_data['year'] <= validation_year - 1)]['ret']\n",
    "\n",
    "        # 2. Validation 데이터: 해당 validation_year\n",
    "        validation_X = X_data[X_data['year'] == validation_year].drop(columns=['year', 'month'])\n",
    "        validation_y = y_data[y_data['year'] == validation_year]['ret']\n",
    "\n",
    "        # 3. XGBoost 모델 정의\n",
    "        model = lgb.LGBMRegressor(random_state = 42)\n",
    "        train_X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "        # 데이터프레임 컬럼 이름\n",
    "        columns = train_X.columns\n",
    "\n",
    "        # 허용되지 않는 특수 문자 패턴 정의\n",
    "        invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "        # 문제 있는 컬럼 이름 탐지\n",
    "        invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "        train_X.columns = train_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "        validation_X.columns = validation_X.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "        model.fit(train_X, train_y)  # 모델 학습\n",
    "\n",
    "        # 4. Validation 예측\n",
    "        y_pred = model.predict(validation_X)\n",
    "\n",
    "        # 5. 성능 측정 (MSE)\n",
    "        mse = mean_squared_error(validation_y, y_pred)\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append(mse)\n",
    "\n",
    "    # 각 데이터셋의 평균 MSE 계산\n",
    "    avg_mse = np.mean(results)\n",
    "    final_results.append({'Dataset': dataset_name, 'Average MSE': avg_mse})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)\n",
    "\n",
    "# 결과를 CSV로 저장\n",
    "final_results_df.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/58_average_mse_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "re_58 = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/58_average_mse_results.csv')\n",
    "re_AE = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/AE_average_mse_results.csv')\n",
    "re_PCA = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/PCA_average_mse_results.csv')\n",
    "re_UA = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/UA_average_mse_results.csv')\n",
    "\n",
    "\n",
    "A = pd.concat([re_58,re_AE,re_PCA,re_UA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Average MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58F_124E</td>\n",
       "      <td>0.006881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58F_AE10E</td>\n",
       "      <td>0.005783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58F_AE30E</td>\n",
       "      <td>0.006464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58F_AE50E</td>\n",
       "      <td>0.007218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58F_LT10E</td>\n",
       "      <td>0.006394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  Average MSE\n",
       "0   58F_124E     0.006881\n",
       "1  58F_AE10E     0.005783\n",
       "2  58F_AE30E     0.006464\n",
       "3  58F_AE50E     0.007218\n",
       "4  58F_LT10E     0.006394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.sort_values(by = 'Average MSE').head(45).to_csv(\"C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/top45/LGBM_TOP45.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 Grid SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 경로\n",
    "base_path = 'C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/'\n",
    "\n",
    "# 파일 이름 리스트 (확장자 제외)\n",
    "file_names = [\n",
    "    \"58F_UA10E\", \"AE30F_PCA30E\", \"UA30F_UA10E\", \"PCA30F_PCA30E\", \"PCA10F_UA10E\",\n",
    "    \"UA10F_UA10E\", \"PCA10F_PCA30E\", \"AE30F_PCA50E\", \"UA30F_PCA50E\", \"AE10F_UA10E\",\n",
    "    \"AE10F_AE10E\", \"PCA10F_AE10E\", \"PCA30F_UA10E\",\"58F_PCA30E\",\"PCA30F_PCA50E\",\n",
    "    \"58F_AE10E\", \"UA10F_AE10E\", \"AE30F_AE10E\",\"UA10F_UA30E\", \"PCA30F_AE10E\"]\n",
    "\n",
    "# 파일 읽기 및 변수 생성\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}.csv\"  # 파일 경로\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # 변수 생성\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "\n",
    "\n",
    "# 데이터셋 이름 리스트\n",
    "datasets =[\n",
    "    \"58F_UA10E\", \"AE30F_PCA30E\", \"UA30F_UA10E\", \"PCA30F_PCA30E\", \"PCA10F_UA10E\",\n",
    "    \"UA10F_UA10E\", \"PCA10F_PCA30E\", \"AE30F_PCA50E\", \"UA30F_PCA50E\", \"AE10F_UA10E\",\n",
    "    \"AE10F_AE10E\", \"PCA10F_AE10E\", \"PCA30F_UA10E\",\"58F_PCA30E\",\"PCA30F_PCA50E\",\n",
    "    \"58F_AE10E\", \"UA10F_AE10E\", \"AE30F_AE10E\",\"UA10F_UA30E\", \"PCA30F_AE10E\"]\n",
    "\n",
    "\n",
    "# 결과 저장 리스트\n",
    "final_results = []\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200],\n",
    "    'max_depth': [4, 8],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [50,100], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# 모든 데이터셋에 대해 반복\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing: {dataset_name}\")\n",
    "\n",
    "    # X 데이터 로드\n",
    "    X_data = globals()[dataset_name]  # 변수명으로 데이터 로드\n",
    "\n",
    "    # Y 데이터 (ret와 year)\n",
    "    y_data = y.copy()  # ret와 year가 포함된 데이터프레임\n",
    "\n",
    "\n",
    "# 1. Train 데이터: 1997 ~ (validation_year - 1)\n",
    "    train_X = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "    train_y = y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "        \n",
    "        # TimeSeriesSplit 정의\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=tscv,              # TimeSeriesSplit 사용\n",
    "        scoring='neg_mean_squared_error'\n",
    "        )\n",
    "\n",
    "    grid_search.fit(train_X, train_y)\n",
    "\n",
    "    final_results.append({'Dataset' :dataset_name, 'MSE' : grid_search.best_score_, 'Best Parameters' :  grid_search.best_params_})\n",
    "\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# 출력\n",
    "print(\"Final Results:\")\n",
    "print(final_results_df)\n",
    "\n",
    "# 결과를 CSV로 저장\n",
    "final_results_df.to_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/top20result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 100}\n",
      "Best Score: 0.19397772543354566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/AE30F_PCA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 300, 'num_leaves': 200}\n",
      "Best Score: 0.17625255927241815\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/58F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 100, 'num_leaves': 200}\n",
      "Best Score: 0.13435360376316566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/UA30F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 100}\n",
      "Best Score: 0.19975900578436248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA30F_PCA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100, 'num_leaves': 200}\n",
      "Best Score: 0.18497581778439245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA10F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: 0.13586451008858752\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/UA10F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 100}\n",
      "Best Score: 0.20248857577122595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA10F_PCA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 50}\n",
      "Best Score: 0.19666775022709965\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/AE30F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: 0.15830165683700068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/UA30F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 50}\n",
      "Best Score: 0.17613522080104718\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/AE10F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 300, 'num_leaves': 100}\n",
      "Best Score: 0.1712508829835026\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/AE10F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 100}\n",
      "Best Score: 0.17485694123710555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA10F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 200}\n",
      "Best Score: 0.17886427874135563\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA30F_UA10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 300, 'num_leaves': 200}\n",
      "Best Score: 0.21823686616339052\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/58F_PCA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 100}\n",
      "Best Score: 0.2069765822086395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA30F_PCA50E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 300, 'num_leaves': 100}\n",
      "Best Score: 0.16873011572796698\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/58F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: 0.12773508732401123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/UA10F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 300, 'num_leaves': 100}\n",
      "Best Score: 0.16935150796150017\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/AE30F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 300, 'num_leaves': 50}\n",
      "Best Score: 0.13875750918875557\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/UA10F_UA30E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 100}\n",
      "Best Score: 0.1696173196750425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA30F_AE10E.csv')\n",
    "X_data = X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "y_data =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [2,8],\n",
    "    'learning_rate': [0.01, 0.05,0.1],\n",
    "    'num_leaves': [50,100,200], #트리의 최대 노드 갯수 : 2^(max_depth) 보다 작거나 같은 값\n",
    "}\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# Custom Scorer 생성 함수\n",
    "def adj_r2(estimator, X, y_true):\n",
    "    n, p = X.shape\n",
    "    pred = estimator.predict(X)\n",
    "    return 1 - ((1 - r2_score(y_true, pred)) * (n - 1))/(n-p-1)\n",
    "\n",
    "# Scorer를 make_scorer로 생성\n",
    "scorer = make_scorer(adj_r2, greater_is_better=True)\n",
    "\n",
    "\n",
    "# GridSearchCV 실행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,              # TimeSeriesSplit 사용\n",
    "    scoring = {\n",
    "       \"adj R2\": adj_r2  \n",
    "    },\n",
    "    refit=\"adj R2\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_data, y_data)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted r2 :  -0.11894732124808027\n",
      "R2 score :  -0.10042170334662193\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "firm=pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/차원축소데이터_1202/firm_500_new.csv')\n",
    "y=firm[['ret','year']]\n",
    "\n",
    "\n",
    "\n",
    "X_data = pd.read_csv('C:/Users/DaBin/Desktop/대학원/2024-2/통계계산특론/PROJECT/data/Top20/PCA30F_PCA50E.csv')\n",
    "X_train= X_data[(X_data['year'] >= 1997) & (X_data['year'] <= 2016)].drop(columns=['year', 'month'])\n",
    "X_test= X_data[(X_data['year'] == 2017) ].drop(columns=['year', 'month'])\n",
    "y_train =y[(y['year'] >= 1997) & (y['year'] <= 2016)]['ret']\n",
    "y_test =y[y['year'] == 2017 ]['ret']\n",
    "\n",
    "\n",
    "\n",
    "# 데이터프레임 컬럼 이름\n",
    "columns = X_data.columns\n",
    "\n",
    "# 허용되지 않는 특수 문자 패턴 정의\n",
    "invalid_pattern = re.compile(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]')\n",
    "\n",
    "# 문제 있는 컬럼 이름 탐지\n",
    "invalid_columns = [col for col in columns if invalid_pattern.search(col)]\n",
    "X_data.columns = X_data.columns.str.replace(r'[{}[\\]:,\\\\/\"\\'\\s\\n\\t]', '', regex=True)\n",
    "\n",
    "\n",
    "# TimeSeriesSplit 정의\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = lgb.LGBMRegressor(random_state = 42,verbose = -1, learning_rate= 0.05, n_estimators = 200, max_depth = 8, num_leaves = 200)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "print(\"Adjusted r2 : \", adjusted_r2_score(y_test,y_pred, X_test.shape[0], X_test.shape[1]))\n",
    "print(\"R2 score : \", r2_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
